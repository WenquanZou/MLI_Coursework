{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "CW-Brain-Age-Regression.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WenquanZou/MLI_Coursework/blob/CNN/CW-Brain-Age-Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9U-FzmezghE",
        "colab_type": "text"
      },
      "source": [
        "# Coursework: Age regression from brain MRI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsgrZMRMzghF",
        "colab_type": "text"
      },
      "source": [
        "Predicting the age of patient from a brain MRI scan can have diagnostic value for a number of diseases that may cause structural changes and potential damage to the brain. A discrepancy between the predicted age and the real, chronological age of a patient might indicate the presence of disease. This requires an accurate predictor of brain age which may be learned from a set of healthy reference subjects, given their brain MRI data and their actual age.\n",
        "\n",
        "The objective for the coursework is to implement different supervised learning approaches for age regression from brain MRI. We provided data from a total of 652 healthy subjects, that is split into different development sets and a held-out test set on which you will evaluate your final prediction accuracy.\n",
        "\n",
        "Each approach will require a processing pipeline with different components that you will need to implement using methods that were discussed in the lectures and tutorials. There are three dedicated parts in the Jupyter notebook for each approach which contain some detailed instructions and some helper code.\n",
        "\n",
        "You may find some useful ideas and implementations in the tutorial notebooks. Once you have implemented all approaches and obtained results from your experiments, we ask you to write a short summary report. The reports should contain a short introduction, description of each of your methods and the individual processing steps, your results with a brief discussion. The report should also include some figures and plots to support your findings.\n",
        "\n",
        "#### Read the text descriptions and the provided code cells carefully and look out for the cells marked with 'TASK' and 'ADD YOUR CODE HERE'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHrXZgCUzghH",
        "colab_type": "text"
      },
      "source": [
        "### Getting started and familiarise ourselves with the data\n",
        "\n",
        "The following cells provide some helper functions to load the data, and provide some overview and visualisation of the statistics over the total population of 652 subjects. The data will be split into different subsets to be used for different parts of the coursework. There is a set of 52 subjects to be used in part A to develop an image segmentation method (47 for training, 5 for validation). We then use 500 subjects for training and cross-validation of age regression approaches in part A, B and C. A remaining set of 100 subjects is used to test the final age prediction accuracy and will be made available towards the end of the coursework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moCnYOjMzghI",
        "colab_type": "text"
      },
      "source": [
        "### Running on Colab or Azure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4HPUp8PzghK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install SimpleITK==1.2.2 \n",
        "! wget https://www.doc.ic.ac.uk/~bglocker/teaching/notebooks/brainage-data.zip\n",
        "! wget https://raw.githubusercontent.com/WenquanZou/MLI_Coursework/master/meta_data_reg_test.csv\n",
        "! unzip brainage-data.zip\n",
        "\n",
        "# data directory\n",
        "data_dir = 'data/brain_age/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsFClbM6zghO",
        "colab_type": "text"
      },
      "source": [
        "### Running on DoC lab machines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8DDe-ExzghP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data directory\n",
        "# data_dir = '/vol/lab/course/416/data/brain_age/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76w0qz0TzghT",
        "colab_type": "text"
      },
      "source": [
        "Let's start by loading the meta data of the entire population, that is the data containing information about the subject IDs, their age, and gender."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tiar_VpzghV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the meta data using pandas\n",
        "import pandas as pd\n",
        "\n",
        "# data directory\n",
        "data_dir = 'data/brain_age/'\n",
        "meta_data_all = pd.read_csv(data_dir + 'meta/meta_data_all.csv')\n",
        "meta_data_all.head() # show the first five data entries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJKBs6IdzghY",
        "colab_type": "text"
      },
      "source": [
        "Let's have a look at some population statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3iNb8Wunzgha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "meta_data = meta_data_all\n",
        "\n",
        "sns.catplot(x=\"gender_text\", data=meta_data, kind=\"count\")\n",
        "plt.title('Gender distribution')\n",
        "plt.xlabel('Gender')\n",
        "plt.show()\n",
        "\n",
        "sns.distplot(meta_data['age'], bins=[10,20,30,40,50,60,70,80,90])\n",
        "plt.title('Age distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(range(len(meta_data['age'])),meta_data['age'], marker='.')\n",
        "plt.grid()\n",
        "plt.xlabel('Subject')\n",
        "plt.ylabel('Age')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B473UHqMzghe",
        "colab_type": "text"
      },
      "source": [
        "### Set up a simple medical image viewer and import SimpleITK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP3AaIKKzghg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from ipywidgets import interact, fixed\n",
        "from IPython.display import display\n",
        "\n",
        "# Calculate parameters low and high from window and level\n",
        "def wl_to_lh(window, level):\n",
        "    low = level - window/2\n",
        "    high = level + window/2\n",
        "    return low,high\n",
        "\n",
        "def display_image(img, x=None, y=None, z=None, window=None, level=None, colormap='gray', crosshair=False):\n",
        "    # Convert SimpleITK image to NumPy array\n",
        "    img_array = sitk.GetArrayFromImage(img)\n",
        "    \n",
        "    # Get image dimensions in millimetres\n",
        "    size = img.GetSize()\n",
        "    spacing = img.GetSpacing()\n",
        "    width  = size[0] * spacing[0]\n",
        "    height = size[1] * spacing[1]\n",
        "    depth  = size[2] * spacing[2]\n",
        "    \n",
        "    if x is None:\n",
        "        x = np.floor(size[0]/2).astype(int)\n",
        "    if y is None:\n",
        "        y = np.floor(size[1]/2).astype(int)\n",
        "    if z is None:\n",
        "        z = np.floor(size[2]/2).astype(int)\n",
        "    \n",
        "    if window is None:\n",
        "        window = np.max(img_array) - np.min(img_array)\n",
        "    \n",
        "    if level is None:\n",
        "        level = window / 2 + np.min(img_array)\n",
        "    \n",
        "    low,high = wl_to_lh(window,level)\n",
        "\n",
        "    # Display the orthogonal slices\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n",
        "\n",
        "    ax1.imshow(img_array[z,:,:], cmap=colormap, clim=(low, high), extent=(0, width, height, 0))\n",
        "    ax2.imshow(img_array[:,y,:], origin='lower', cmap=colormap, clim=(low, high), extent=(0, width,  0, depth))\n",
        "    ax3.imshow(img_array[:,:,x], origin='lower', cmap=colormap, clim=(low, high), extent=(0, height, 0, depth))\n",
        "\n",
        "    # Additionally display crosshairs\n",
        "    if crosshair:\n",
        "        ax1.axhline(y * spacing[1], lw=1)\n",
        "        ax1.axvline(x * spacing[0], lw=1)\n",
        "        ax2.axhline(z * spacing[2], lw=1)\n",
        "        ax2.axvline(x * spacing[0], lw=1)\n",
        "        ax3.axhline(z * spacing[2], lw=1)\n",
        "        ax3.axvline(y * spacing[1], lw=1)\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "def interactive_view(img):\n",
        "    size = img.GetSize() \n",
        "    img_array = sitk.GetArrayFromImage(img)\n",
        "    interact(display_image,img=fixed(img),\n",
        "             x=(0, size[0] - 1),\n",
        "             y=(0, size[1] - 1),\n",
        "             z=(0, size[2] - 1),\n",
        "             window=(0,np.max(img_array) - np.min(img_array)),\n",
        "             level=(np.min(img_array),np.max(img_array)));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzoPT7V9zghk",
        "colab_type": "text"
      },
      "source": [
        "### Imaging data\n",
        "\n",
        "Let's check out the imaging data that is available for each subject. This cell also shows how to retrieve data given a particular subject ID from the meta data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTjD2ozSzghl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subject with index 0\n",
        "ID = meta_data['subject_id'][0]\n",
        "age = meta_data['age'][0]\n",
        "\n",
        "# Image\n",
        "image_filename = data_dir + 'images/sub-' + ID + '_T1w_unbiased.nii.gz'\n",
        "img = sitk.ReadImage(image_filename)\n",
        "\n",
        "# Mask\n",
        "mask_filename = data_dir + 'masks/sub-' + ID + '_T1w_brain_mask.nii.gz'\n",
        "msk = sitk.ReadImage(mask_filename)\n",
        "\n",
        "# Grey matter map\n",
        "gm_filename = data_dir + 'greymatter/wc1sub-' + ID + '_T1w.nii.gz'\n",
        "gm = sitk.ReadImage(gm_filename)\n",
        "\n",
        "print('Imaging data of subject ' + ID + ' with age ' + str(age))\n",
        "\n",
        "print('\\nMR Image (used in part A)')\n",
        "display_image(img, window=400, level=200)\n",
        "\n",
        "print('Brain mask (used in part A)')\n",
        "display_image(msk)\n",
        "\n",
        "print('Spatially normalised grey matter maps (used in part B and C)')\n",
        "display_image(gm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYe8vsJ2zghp",
        "colab_type": "text"
      },
      "source": [
        "## Part A: Volume-based regression using brain structure segmentation\n",
        "\n",
        "The first approach aims to regress the age of a subject using the volumes of brain tissues as features. The brain structures include grey matter (GM), white matter (WM), and cerebrospinal fluid (CSF). It is known that with increasing age the ventricles enlarge (filled with CSF), while it is assumed that grey and white matter volume may decrease over time. However, as overall brain volume varies across individuals, taking the absolute volumes of tissues might not be predictive. Instead, relative volumes need to be computed as the ratios between each tissue volume and overall brain volume. To this end, a four-class (GM, WM, CSF, and background) brain segmentation needs to be implemented which will be trained using a total of 52 subjects (47 for training, 5 for validation). The segmentation method is then applied to the remaining 600 brain scans which will be used to train and test the age regression. Brain masks are provided which have been generated with a state-of-the-art neuroimaging brain extraction tool.\n",
        "\n",
        "Different regression techniques should be explored, and it might be beneficial to investigate what the best set of features is for this task. Are all volume features equally useful, or is it even better to combine some of them and create new features. How does a simple linear regression perform compared to a model with higher order polynomials? Do you need regularisation? How about other regression methods such as regression trees, SVMs or neural networks? The accuracy of different methods should be evaluated using two-fold cross-validation on the set of 500 subjects, and average age prediction accuracy should be compared and reported appropriately. The final prediction accuracy will be evaluated on a hold-out set of 100 subjects.\n",
        "\n",
        "*Note:* For part A, only the MR images and the brain masks should be used from the imaging data. The spatially normalised grey matter maps are used in part B and C only. If you struggle with task A-1, you can continue with A-2 using the provided reference segmentations in subfolder `segs_refs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He9qtH3Hzghq",
        "colab_type": "text"
      },
      "source": [
        "### TASK A-1: Brain tissue segmentation\n",
        "\n",
        "Implement a CNN model for brain tissue segmentation which can provide segmentations of GM, WM, and CSF. For this task (and only for this task), we provide a subset of 52 subjects which are split into 47 images for training and 5 for validation. The template code below has the data handling and main training routines already implemented, so you can focus on implementing a suitable CNN model. A simple model is provided, but this won't perform very well.\n",
        "\n",
        "Once your model is trained and you are happy with the results on the validation data you should apply it to the 500 subjects later used for training the age regressor. We provide reference segmentations in a subfolder `segs_refs` for all subjects. Calculate Dice similarity coefficients per tissue when comparing your predicted segmentations to the reference segmentations. Summarise the statistics of the 500 Dice scores for each tissue class in [box-and-whisker-plots](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.boxplot.html).\n",
        "\n",
        "*Note:* Implementing a full-fledged machine learning pipeline with training and testing procedures in Jupyter notebooks is a bit cumbersome and a pain to debug. Also, running bigger training tasks can be unstable. The code below should work as is on your VM. However, if you want to get a bit more serious about implementing an advanced CNN approach for image segmentation, you may want to move code into separate Python scripts and run these from the terminal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmUpXCVTzghr",
        "colab_type": "text"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cKxDlt-zght",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyeQNhKizghx",
        "colab_type": "text"
      },
      "source": [
        "#### Data Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNvEF7Ojzghy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_mean_unit_var(image, mask):\n",
        "    \"\"\"Normalizes an image to zero mean and unit variance.\"\"\"\n",
        "\n",
        "    img_array = sitk.GetArrayFromImage(image)\n",
        "    img_array = img_array.astype(np.float32)\n",
        "\n",
        "    msk_array = sitk.GetArrayFromImage(mask)\n",
        "\n",
        "    mean = np.mean(img_array[msk_array>0])\n",
        "    std = np.std(img_array[msk_array>0])\n",
        "\n",
        "    if std > 0:\n",
        "        img_array = (img_array - mean) / std\n",
        "        img_array[msk_array==0] = 0\n",
        "\n",
        "    image_normalised = sitk.GetImageFromArray(img_array)\n",
        "    image_normalised.CopyInformation(image)\n",
        "\n",
        "    return image_normalised\n",
        "\n",
        "\n",
        "def resample_image(image, out_spacing=(1.0, 1.0, 1.0), out_size=None, is_label=False, pad_value=0):\n",
        "    \"\"\"Resamples an image to given element spacing and output size.\"\"\"\n",
        "\n",
        "    original_spacing = np.array(image.GetSpacing())\n",
        "    original_size = np.array(image.GetSize())\n",
        "\n",
        "    if out_size is None:\n",
        "        out_size = np.round(np.array(original_size * original_spacing / np.array(out_spacing))).astype(int)\n",
        "    else:\n",
        "        out_size = np.array(out_size)\n",
        "\n",
        "    original_direction = np.array(image.GetDirection()).reshape(len(original_spacing),-1)\n",
        "    original_center = (np.array(original_size, dtype=float) - 1.0) / 2.0 * original_spacing\n",
        "    out_center = (np.array(out_size, dtype=float) - 1.0) / 2.0 * np.array(out_spacing)\n",
        "\n",
        "    original_center = np.matmul(original_direction, original_center)\n",
        "    out_center = np.matmul(original_direction, out_center)\n",
        "    out_origin = np.array(image.GetOrigin()) + (original_center - out_center)\n",
        "\n",
        "    resample = sitk.ResampleImageFilter()\n",
        "    resample.SetOutputSpacing(out_spacing)\n",
        "    resample.SetSize(out_size.tolist())\n",
        "    resample.SetOutputDirection(image.GetDirection())\n",
        "    resample.SetOutputOrigin(out_origin.tolist())\n",
        "    resample.SetTransform(sitk.Transform())\n",
        "    resample.SetDefaultPixelValue(pad_value)\n",
        "\n",
        "    if is_label:\n",
        "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
        "    else:\n",
        "        resample.SetInterpolator(sitk.sitkBSpline)\n",
        "\n",
        "    return resample.Execute(image)\n",
        "\n",
        "\n",
        "class ImageSegmentationDataset(Dataset):\n",
        "    \"\"\"Dataset for image segmentation.\"\"\"\n",
        "\n",
        "    def __init__(self, file_list_img, file_list_seg, file_list_msk, img_spacing, img_size):\n",
        "        self.samples = []\n",
        "        self.img_names = []\n",
        "        self.seg_names = []\n",
        "        for idx, _ in enumerate(tqdm(range(len(file_list_img)), desc='Loading Data')):\n",
        "            img_path = file_list_img[idx]\n",
        "            seg_path = file_list_seg[idx]\n",
        "            msk_path = file_list_msk[idx]\n",
        "\n",
        "            img = sitk.ReadImage(img_path, sitk.sitkFloat32)\n",
        "\n",
        "            seg = sitk.ReadImage(seg_path, sitk.sitkInt64)\n",
        "\n",
        "            msk = sitk.ReadImage(msk_path, sitk.sitkUInt8)\n",
        "\n",
        "            #pre=processing\n",
        "            img = zero_mean_unit_var(img, msk)\n",
        "            img = resample_image(img, img_spacing, img_size, is_label=False)\n",
        "            seg = resample_image(seg, img_spacing, img_size, is_label=True)\n",
        "            msk = resample_image(msk, img_spacing, img_size, is_label=True)\n",
        "\n",
        "            sample = {'img': img, 'seg': seg, 'msk': msk}\n",
        "\n",
        "            self.samples.append(sample)\n",
        "            self.img_names.append(os.path.basename(img_path))\n",
        "            self.seg_names.append(os.path.basename(seg_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sample = self.samples[item]\n",
        "\n",
        "        image = torch.from_numpy(sitk.GetArrayFromImage(sample['img'])).unsqueeze(0)\n",
        "        seg = torch.from_numpy(sitk.GetArrayFromImage(sample['seg'])).unsqueeze(0)\n",
        "        msk = torch.from_numpy(sitk.GetArrayFromImage(sample['msk'])).unsqueeze(0)\n",
        "\n",
        "        return {'img': image, 'seg': seg, 'msk': msk}\n",
        "\n",
        "    def get_sample(self, item):\n",
        "        return self.samples[item]\n",
        "\n",
        "    def get_img_name(self, item):\n",
        "        return self.img_names[item]\n",
        "\n",
        "    def get_seg_name(self, item):\n",
        "        return self.seg_names[item]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-L5giOezgh1",
        "colab_type": "text"
      },
      "source": [
        "#### Check that the GPU is up and running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH66sMpnzgh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda_dev = '0' #GPU device 0 (can be changed if multiple GPUs are available)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:\" + cuda_dev if use_cuda else \"cpu\")\n",
        "\n",
        "print('Device: ' + str(device))\n",
        "if use_cuda:\n",
        "    print('GPU: ' + str(torch.cuda.get_device_name(int(cuda_dev))))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVEfS9iKzgh6",
        "colab_type": "text"
      },
      "source": [
        "#### Config and hyper-parameters\n",
        "\n",
        "Here we set some default hyper-parameters and a starting configuration for the image resolution and others.\n",
        "\n",
        "**TASK: This needs to be revisited to optimise these values. In particular, you may want to run your final model on higher resolution images.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGJKVHCQzgh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnd_seed = 42 #fixed random seed\n",
        "\n",
        "img_size = [64, 64, 64]\n",
        "img_spacing = [3, 3, 3]\n",
        "\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 2\n",
        "val_interval = 10\n",
        "\n",
        "num_classes = 4\n",
        "\n",
        "out_dir = './output'\n",
        "\n",
        "# Create output directory\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR0apeY0zgh_",
        "colab_type": "text"
      },
      "source": [
        "#### Loading and pre-processing of training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlJX9_CMzgiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_data_seg_train = pd.read_csv(data_dir + 'meta/meta_data_seg_train.csv')\n",
        "ids_seg_train = list(meta_data_seg_train['subject_id'])\n",
        "files_seg_img_train = [data_dir + 'images/sub-' + f + '_T1w_unbiased.nii.gz' for f in ids_seg_train]\n",
        "files_seg_seg_train = [data_dir + 'segs_refs/sub-' + f + '_T1w_seg.nii.gz' for f in ids_seg_train]\n",
        "files_seg_msk_train = [data_dir + 'masks/sub-' + f + '_T1w_brain_mask.nii.gz' for f in ids_seg_train]\n",
        "\n",
        "meta_data_seg_val = pd.read_csv(data_dir + 'meta/meta_data_seg_val.csv')\n",
        "ids_seg_val = list(meta_data_seg_val['subject_id'])\n",
        "files_seg_img_val = [data_dir + 'images/sub-' + f + '_T1w_unbiased.nii.gz' for f in ids_seg_val]\n",
        "files_seg_seg_val = [data_dir + 'segs_refs/sub-' + f + '_T1w_seg.nii.gz' for f in ids_seg_val]\n",
        "files_seg_msk_val = [data_dir + 'masks/sub-' + f + '_T1w_brain_mask.nii.gz' for f in ids_seg_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z11zHlIGzgiE",
        "colab_type": "text"
      },
      "source": [
        "We apply some standard pre-processing on the data such as intensity normalization (zero mean unit variance) and downsampling according to the configuration above.\n",
        "\n",
        "**You may want to use initially the validation data with 5 subjects for training which is more efficient when debugging your training routine and model implementation. Make sure to later train your final model on the actual training data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rss7YXgAzgiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOAD ACTUAL TRAINING DATA\n",
        "dataset_train = ImageSegmentationDataset(files_seg_img_train, files_seg_seg_train, files_seg_msk_train, img_spacing, img_size)\n",
        "# LOAD VALIDATION DATA AS TRAINING FOR QUICK DEBUGGING\n",
        "# dataset_train = ImageSegmentationDataset(files_seg_img_val, files_seg_seg_val, files_seg_msk_val, img_spacing, img_size)\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataset_val = ImageSegmentationDataset(files_seg_img_val, files_seg_seg_val, files_seg_msk_val, img_spacing, img_size)\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMEpXXf6zgiJ",
        "colab_type": "text"
      },
      "source": [
        "#### Visualise training example\n",
        "\n",
        "Just to check how a training image looks like after pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSIKyPGAzgiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = dataset_train.get_sample(0)\n",
        "img_name = dataset_train.get_img_name(0)\n",
        "seg_name = dataset_train.get_seg_name(0)\n",
        "print('Image: ' + img_name)\n",
        "display_image(sample['img'], window=5, level=0)\n",
        "print('Segmentation')\n",
        "display_image(sitk.LabelToRGB(sample['seg']))\n",
        "print('Mask')\n",
        "display_image(sample['msk'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iCRT63rzgiN",
        "colab_type": "text"
      },
      "source": [
        "#### The Model\n",
        "\n",
        "**TASK:** This is the **key part of task A-1** where you have to design a suitable CNN model for brain segmentation. The simple model provided below works to some degree (it let's you run through the upcoming cells), but it will not perform very well. Use what you learned in the lectures to come up with a good architecture. Start with a simple, shallow model and only increase complexity (e.g., number of layers) if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3nKiLSazgiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "# BUILD A BETTER MODEL HERE\n",
        "########################################\n",
        "\n",
        "class SimpleNet3D(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleNet3D, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv3d(64, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv3d(32, 16, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv3d(16, num_classes, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.conv4(x)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB210VEVzgiT",
        "colab_type": "text"
      },
      "source": [
        "#### TRAINING\n",
        "\n",
        "Below is an implementation of a full training procedure including a loop for intermediate evaluation of the model on the validation data. Feel free to modify this procedure. For example, in addition to the loss you may want to monitor precision, recall and Dice scores (or others)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll9r1PqwrEtb",
        "colab_type": "text"
      },
      "source": [
        "### Model loading\n",
        "Load trained model so we do not need to retrained it again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTrFPMMSrBdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = os.path.join(out_dir, 'model')\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "model_path = os.path.join(model_dir, 'model.pt')\n",
        "if os.path.exists(model_path):\n",
        "    model = SimpleNet3D(num_classes=num_classes)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AgioJfcdzgiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(rnd_seed) #fix random seed\n",
        "\n",
        "model = SimpleNet3D(num_classes=num_classes).to(device)\n",
        "model.train()\n",
        "    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_train_log = []\n",
        "loss_val_log = []\n",
        "epoch_val_log = []\n",
        "    \n",
        "print('START TRAINING...')\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    # Training\n",
        "    for batch_idx, batch_samples in enumerate(dataloader_train):\n",
        "        img, seg = batch_samples['img'].to(device), batch_samples['seg'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        prd = model(img)\n",
        "        prd_flat = prd.view(prd.size(0), prd.size(1), -1)\n",
        "        seg_flat = seg.view(seg.size(0), seg.size(1), -1)\n",
        "        loss = F.cross_entropy(prd_flat, seg_flat.squeeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_train_log.append(loss.item())\n",
        "\n",
        "    print('+ TRAINING \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
        "    \n",
        "    # Validation\n",
        "    if epoch == 1 or epoch % val_interval == 0:\n",
        "        loss_val = 0\n",
        "        sum_pts = 0\n",
        "        with torch.no_grad():\n",
        "            for data_sample in dataloader_val:\n",
        "                img, seg = data_sample['img'].to(device), data_sample['seg'].to(device)\n",
        "                prd = model(img)\n",
        "                prd_flat = prd.view(prd.size(0), prd.size(1), -1)\n",
        "                seg_flat = seg.view(seg.size(0), seg.size(1), -1)\n",
        "                loss_val += F.cross_entropy(prd_flat, seg_flat.squeeze(1), reduction='sum').item()\n",
        "                sum_pts += seg_flat.size(2)\n",
        "                \n",
        "        prd = torch.argmax(prd, dim=1)\n",
        "        prediction = sitk.GetImageFromArray(prd.cpu().squeeze().numpy().astype(np.uint8))\n",
        "        \n",
        "\n",
        "        loss_val /= sum_pts\n",
        "\n",
        "        loss_val_log.append(loss_val)\n",
        "        epoch_val_log.append(epoch)\n",
        "\n",
        "        print('--------------------------------------------------')\n",
        "        print('+ VALIDATE \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, loss_val))\n",
        "        display_image(sitk.LabelToRGB(prediction))\n",
        "        print('--------------------------------------------------')\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(model_dir, 'model.pt'))\n",
        "\n",
        "print('\\nFinished TRAINING.')\n",
        "\n",
        "plt.plot(range(1, num_epochs + 1), loss_train_log, c='r', label='train')\n",
        "plt.plot(epoch_val_log, loss_val_log, c='b', label='val')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uibqYmXSzgiX",
        "colab_type": "text"
      },
      "source": [
        "#### Loading and pre-processing of testing data\n",
        "\n",
        "Now that we have trained a model, the next cells are about applying that model to the 500 subjects that are used for training the age regressor. Note, at a later stage you will also need to run the model on the 100 subjects from the hold-out set, once these have been made available. Before testing on the full set, you may want to initially just test on the 5 validation subjects to check everything is working fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKXYrOt8zgiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_data_reg_test = pd.read_csv('meta_data_reg_test.csv')\n",
        "ids_seg_test = list(meta_data_reg_test['subject_id'])\n",
        "files_seg_img_test = [data_dir + 'images/sub-' + f + '_T1w_unbiased.nii.gz' for f in ids_seg_test]\n",
        "files_seg_seg_test = [data_dir + 'segs_refs/sub-' + f + '_T1w_seg.nii.gz' for f in ids_seg_test]\n",
        "files_seg_msk_test = [data_dir + 'masks/sub-' + f + '_T1w_brain_mask.nii.gz' for f in ids_seg_test]\n",
        "\n",
        "dataset_test = ImageSegmentationDataset(files_seg_img_test, files_seg_seg_test, files_seg_msk_test, img_spacing, img_size)\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwA5bJjMzgic",
        "colab_type": "text"
      },
      "source": [
        "#### Visualise testing example\n",
        "\n",
        "Just to check how a testing image looks like after pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x19wKK0lzgid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = dataset_test.get_sample(0)\n",
        "img_name = dataset_test.get_img_name(0)\n",
        "seg_name = dataset_test.get_seg_name(0)\n",
        "print('Image: ' + img_name)\n",
        "display_image(sample['img'], window=5, level=0)\n",
        "print('Segmentation')\n",
        "display_image(sitk.LabelToRGB(sample['seg']))\n",
        "print('Mask')\n",
        "display_image(sample['msk'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4E62CM2zgig",
        "colab_type": "text"
      },
      "source": [
        "#### TESTING\n",
        "\n",
        "Below is an implementation of a full testing procedure that saves the segmentations in an output folder. Feel free to modify this procedure.\n",
        "\n",
        "**TASK: You will need to add the calculations of Dice scores (and possibly others) to evaluate the segmentation performance.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ty6ctX9azgih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_dir = os.path.join(out_dir, 'pred')\n",
        "if not os.path.exists(pred_dir):\n",
        "    os.makedirs(pred_dir)\n",
        "\n",
        "model = SimpleNet3D(num_classes=num_classes)\n",
        "model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pt')))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "    \n",
        "print('START TESTING...')\n",
        "\n",
        "loss_test = 0\n",
        "sum_pts = 0\n",
        "idx_test = 0\n",
        "\n",
        "CSF_dice_scores = []\n",
        "GM_dice_scores = []\n",
        "WM_dice_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data_sample in dataloader_test:\n",
        "        img, seg = data_sample['img'].to(device), data_sample['seg'].to(device)\n",
        "        prd = model(img)\n",
        "        prd_flat = prd.view(prd.size(0), prd.size(1), -1)\n",
        "        seg_flat = seg.view(seg.size(0), seg.size(1), -1)\n",
        "        loss_test += F.cross_entropy(prd_flat, seg_flat.squeeze(1), reduction='sum').item()\n",
        "        sum_pts += seg_flat.size(2)        \n",
        "        \n",
        "        prd = torch.argmax(prd, dim=1)\n",
        "\n",
        "        sample = dataset_test.get_sample(idx_test)\n",
        "        name = dataset_test.get_seg_name(idx_test)\n",
        "        prediction = sitk.GetImageFromArray(prd.cpu().squeeze().numpy().astype(np.uint8))\n",
        "        prediction.CopyInformation(sample['seg'])\n",
        "        sitk.WriteImage(prediction, os.path.join(pred_dir, name))\n",
        "        \n",
        "        idx_test += 1\n",
        "\n",
        "        # Get image in array form \n",
        "        ref_img = sample['seg']\n",
        "        pred_img = prediction\n",
        "        ref_img_arr = sitk.GetArrayFromImage(ref_img)\n",
        "        pred_img_arr = sitk.GetArrayFromImage(pred_img)\n",
        "\n",
        "        # Compute overlap measures between same set of labels of pixels of two images\n",
        "        overlap = sitk.LabelOverlapMeasuresImageFilter()\n",
        "        for i, segment_dice_scores in enumerate([CSF_dice_scores, GM_dice_scores, WM_dice_scores]):\n",
        "          \n",
        "          # Dice score calculation for one segmentation\n",
        "          ref_segment_arr = (ref_img_arr == (i + 1)).astype(int)\n",
        "          pred_segment_arr = (pred_img_arr == (i + 1)).astype(int)\n",
        "          ref_segment = sitk.GetImageFromArray(ref_segment_arr)\n",
        "          pred_segment = sitk.GetImageFromArray(pred_segment_arr)\n",
        "          \n",
        "          overlap.Execute(ref_segment, pred_segment)\n",
        "          current_dice_score = overlap.GetDiceCoefficient()\n",
        "          segment_dice_scores.append(current_dice_score)\n",
        "\n",
        "loss_test /= sum_pts\n",
        "\n",
        "print('+ TESTING \\tLoss: {:.6f}'.format(loss_test))\n",
        "\n",
        "# Show last testing sample as an example\n",
        "print('\\n\\nReference segmentation')\n",
        "display_image(sitk.LabelToRGB(sample['seg']))\n",
        "print('Predicted segmentation')\n",
        "display_image(sitk.LabelToRGB(prediction))\n",
        "\n",
        "\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(17.5, 5))\n",
        "f.suptitle('600 Dice Scores for each segementation', fontsize=15)\n",
        "ax1.boxplot(CSF_dice_scores)\n",
        "ax2.boxplot(GM_dice_scores)\n",
        "ax3.boxplot(WM_dice_scores)\n",
        "ax1.set_title('CSF Dice Score')\n",
        "ax2.set_title('GM Dice Score')\n",
        "ax3.set_title('WM Dice Score')\n",
        "print('\\nFinished TESTING.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS61NLPtzgik",
        "colab_type": "text"
      },
      "source": [
        "### TASK A-2: Feature calculation\n",
        "\n",
        "Start by calculating the three absolute tissue volumes for each subject. Plot the volumes against the subjects' ages. Taking the absolute volumes of tissues as features, however, might not be predictive. Instead, relative volumes need to be computed as the ratios between each tissue volume and overall brain volume. But you might also want to explore using different combinations or even polynomial features.\n",
        "\n",
        "Implement a function that constructs a big matrix $X$ with a row for each subject and features across the columns. Start with just calculating three simple features of relative tissue volumes for GM, WM and CSF, and compare these to the absolute volumes plotted above.\n",
        "\n",
        "*Note:* If you are struggling with the previous task on image segmentation, or if you prefer to work on this and the following tasks first, you can continue here using the provided reference segmentations which can be found in a subfolder `segs_refs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpYqr8smzgil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CALCULATE ABSOLUTE TISSUE VOLUMES\n",
        "\n",
        "import os\n",
        "\n",
        "# USE THIS TO RUN THE CALCULATIONS ON YOUR SEGMENTATONS\n",
        "seg_dir = './output/pred/'\n",
        "\n",
        "# USE THIS TO RUN THE CALCULATIONS ON OUR REFERENCE SEGMENTATIONS\n",
        "# seg_dir = data_dir + 'segs_refs/'\n",
        "\n",
        "meta_data_reg_train = pd.read_csv(data_dir + 'meta/meta_data_reg_train.csv')\n",
        "ids_reg_train = list(meta_data_reg_train['subject_id'])\n",
        "files_reg_seg_train = [seg_dir + 'sub-' + f + '_T1w_seg.nii.gz' for f in ids_reg_train]\n",
        "\n",
        "# THIS MATRIX WILL STORE THE VOLUMES PER TISSUE CLASS\n",
        "vols = np.zeros((3,len(files_reg_seg_train)))\n",
        "\n",
        "for idx, _ in enumerate(tqdm(range(len(files_reg_seg_train)), desc='Calculating Features')):\n",
        "    \n",
        "    seg_filename = files_reg_seg_train[idx]\n",
        "    \n",
        "    if os.path.exists(seg_filename):\n",
        "        seg = sitk.ReadImage(seg_filename)\n",
        "        \n",
        "        ########################################\n",
        "        # ADD YOUR CODE HERE\n",
        "        ########################################\n",
        "        seg_array = sitk.GetArrayFromImage(seg)\n",
        "        \n",
        "        unique, counts = np.unique(seg_array, return_counts=True)\n",
        "            \n",
        "        vols[0][idx] = counts[1]\n",
        "        vols[1][idx] = counts[2]\n",
        "        vols[2][idx] = counts[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEwRNddszgio",
        "colab_type": "text"
      },
      "source": [
        "Plot features versus age."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfYYhrDwzgip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(vols[0,:],meta_data_reg_train['age'], marker='.')\n",
        "plt.scatter(vols[1,:],meta_data_reg_train['age'], marker='.')\n",
        "plt.scatter(vols[2,:],meta_data_reg_train['age'], marker='.')\n",
        "plt.grid()\n",
        "plt.title('Unnormalised')\n",
        "plt.xlabel('Volume')\n",
        "plt.ylabel('Age')\n",
        "plt.legend(('CSF','GM','WM'))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwbJZZK6zgir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CALCULATE RELATIVE TISSUE VOLUMES\n",
        "\n",
        "vols_normalised = np.zeros((3,len(files_reg_seg_train)))\n",
        "\n",
        "########################################\n",
        "# ADD YOUR CODE HERE\n",
        "########################################\n",
        "for idx in range(0, vols.shape[1]):\n",
        "    sum = np.sum(vols[:,idx])\n",
        "    vols_normalised[:,idx] = vols[:,idx] / sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RcFCokwzgiu",
        "colab_type": "text"
      },
      "source": [
        "Plot normalised features versus age."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vueCR1nzgiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(vols_normalised[0,:],meta_data_reg_train['age'], marker='.')\n",
        "plt.scatter(vols_normalised[1,:],meta_data_reg_train['age'], marker='.')\n",
        "plt.scatter(vols_normalised[2,:],meta_data_reg_train['age'], marker='.')\n",
        "plt.grid()\n",
        "plt.title('Normalised')\n",
        "plt.xlabel('Volume')\n",
        "plt.ylabel('Age')\n",
        "plt.legend(('CSF','GM','WM'))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8tu5tJXzgiy",
        "colab_type": "text"
      },
      "source": [
        "Final data for age regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_L6WPYbzgiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = vols_normalised.T\n",
        "y = meta_data_reg_train['age'].values.reshape(-1,1)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZOYdpNQzgi5",
        "colab_type": "text"
      },
      "source": [
        "### TASK A-3: Age regression and cross-validation\n",
        "\n",
        "Experiment with different regression methods from the [scikit-learn toolkit](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning). Remember to construct the output vectur $y$ containing the age for each of the subjects.\n",
        "\n",
        "Evaluate the methods using two-fold [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) where the dataset of 500 subjects is split into two equally sized sets $(X_1,y_1)$ and $(X_2,y_2)$ which are used for training and testing in an alternating way (so each set is used as $(X_{\\text{train}},y_{\\text{train}})$ and $(X_{\\text{test}},y_{\\text{test}})$ exactly once).\n",
        "\n",
        "Try using at least three different regression methods, and generate a plot allows easy comparison of the performance of the three methods. Useful [error metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) to report include mean absolute error and r2 score. You might also want to plot the real vs predicted ages.\n",
        "\n",
        "*Note:* These [scikit-learn examples](https://scikit-learn.org/stable/auto_examples/) might serve as an inspiration.\n",
        "\n",
        "*Hint:* Be careful how you split the dataset into two folds. Take into account the data characteristics shown at the top of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcpgRrtTzgi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "# ADD YOUR CODE HERE\n",
        "########################################\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T1fUQCgmx-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "\n",
        "# Linear regression\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(train_X, train_y)\n",
        "predictions.append(reg.predict(test_X))\n",
        "mae_reg = mean_absolute_error(test_y, predictions[0])\n",
        "r2_reg = r2_score(test_y, predictions[0])\n",
        "print(f'Linear regression: mean absolute error: {mae_reg}, r2 score: {r2_reg}')\n",
        "\n",
        "# SVR with polynomial\n",
        "reg_poly = SVR(kernel='poly')\n",
        "reg_poly.fit(train_X, train_y)\n",
        "predictions.append(reg_poly.predict(test_X))\n",
        "mae_svr = mean_absolute_error(test_y, predictions[1])\n",
        "r2_svr  = r2_score(test_y, predictions[1])\n",
        "print(f'SVR of polynomial: mean absolute error: {mae_svr}, r2 score: {r2_svr}')\n",
        "\n",
        "# Lasso regression\n",
        "reg_br = linear_model.BayesianRidge()\n",
        "reg_br.fit(train_X, train_y)\n",
        "predictions.append(reg_br.predict(test_X))\n",
        "mae_br = mean_absolute_error(test_y, predictions[2])\n",
        "r2_br  = r2_score(test_y, predictions[2])\n",
        "print(f'SVR of polynomial: mean absolute error: {mae_br}, r2 score: {r2_br}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtF1jvKAzgi9",
        "colab_type": "text"
      },
      "source": [
        "Error calculation and plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhzfPNn3zgi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(9, 3), sharey=True)\n",
        "methods = ['Linear Regression', 'Polynomial SVR', 'Bayesian Ridge']\n",
        "for i in range(3):\n",
        "  ax[i].scatter(test_y, predictions[i], marker='.')\n",
        "  ax[i].plot([test_y.min(), test_y.max()], [test_y.min(), test_y.max()], 'k--', lw=2)\n",
        "  ax[i].set_xlabel('Real Age')\n",
        "  ax[i].set_ylabel('Predicted Age')\n",
        "  ax[i].set_title(methods[i])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y06qDBGNzgjB",
        "colab_type": "text"
      },
      "source": [
        "## Part B: PCA-based regression using grey matter maps\n",
        "\n",
        "The second approach will make use of grey matter maps that have been already extracted from the MRI scans and aligned to a common reference space to obtain spatially normalised maps. For this, we have used an advanced, state-of-the-art neuroimaging toolkit, called SPM12. The reference space corresponds to the commonly used MNI atlas as seen in the lecture on image segmentation.\n",
        "\n",
        "Because these grey matter maps are spatially normalised (ie., registered), voxel locations across images from different subjects roughly correspond to the same anatomical locations. This means that each voxel location in the grey matter maps can be treated as an individual feature. Because those maps are quite large at their full resolution there would be a very large number of features to deal with (more than 850,000). A dimensionality reduction may need to be performed before training a suitable regressor on the low-dimensional feature representation. We will use Principal Component Analysis (PCA) to do the dimensionality reduction. It might also be beneficial to apply some pre-processing (downsampling, smoothing, etc.) before running PCA, which should be explored. The implemented pipeline should be evaluated using two-fold cross-validation using the same data splits as in part A for the 500 subjects, so the two different approaches can be directly compared in terms average age prediction accuracy.\n",
        "\n",
        "*Note:* For part B, only the spatially normalised grey matter maps should be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg1bVhfIzgjC",
        "colab_type": "text"
      },
      "source": [
        "### TASK B-1: Pre-processing\n",
        "\n",
        "Before running PCA to reduce the dimensionality of the feature space for grey matter maps, it might be beneficial to run some pre-processing on the maps. In voxel-based analysis where each voxel location is a feature, it is common to apply some smoothing beforehand. This is to reduce noise and to compensate for errors of the spatial normalisation.\n",
        "\n",
        "Because the maps are quite large, it might also be worthwile to explore whether downsampling could be performed even before PCA. This would further reduce the dimensionality, and might be even needed in the case where PCA on the orignial resolution runs into memory issues. You may want to consider other ways of pre-processing and you can find insipiration in the notebook on medical image computing `02-Intro-Medical-Image-Computing.ipynb`.\n",
        "\n",
        "Implement a function that performs suitable pre-processing on each grey matter map.\n",
        "\n",
        "*Hint:* You may want to save the pre-processed maps using `sitk.WriteImage` to avoid recomputation each time you run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs4Z1skXzgjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "# ADD YOUR CODE HERE\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLWGs-8JzgjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = img_data\n",
        "y = meta_data_reg_train['age'].values.reshape(-1,1)\n",
        "\n",
        "print(img_size)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFyx2glazgjJ",
        "colab_type": "text"
      },
      "source": [
        "### TASK B-2: Dimensionality reduction\n",
        "\n",
        "Implement dimensionality reduction for grey matter maps using [scitkit-learn's PCA](http://scikit-learn.org/stable/modules/decomposition.html#pca). PCA has an option to set the percentage of variance to be preserved (by setting the parameter `n_components` to a value between 0 and 1). The number of principal modes, that is the new dimensionality of the data, is then automatically determined. Try initially to preserve 95% of the variance (`n_components=0.95`).\n",
        "\n",
        "*Note:* When dimensionality reduction is used as pre-processing step for supervised learning, as in this case, it is important that PCA is fitted to the training data only, but then applied to both the training and testing data. So make sure your implementation consists of two separate steps, 1) fitting the PCA model to $X_{\\text{train}}$ (using the `fit` function), and 2) applying dimensionality reduction to $X_{\\text{train}}$ and $X_{\\text{test}}$ using the `transform` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqAiboDAzgjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "# ADD YOUR CODE HERE\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZoGIVHJzgjN",
        "colab_type": "text"
      },
      "source": [
        "### TASK B-3: Age regression and cross-validation\n",
        "\n",
        "Experiment with different regression methods from the [scikit-learn toolkit](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning). Evaluate the methods using two-fold [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) in the same way as for your approach in Part A so results can be directly compared. Generate the similar plots.\n",
        "\n",
        "Try using at least three different regression methods.\n",
        "\n",
        "*Hint:* Remember, when you use cross-validation where you swap training and testing sets in each fold, you need to fit PCA to the training set of each fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZaakOmuzgjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "# ADD YOUR CODE HERE\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjvLNlNMzgjQ",
        "colab_type": "text"
      },
      "source": [
        "Error calculation and plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqfUGxXDzgjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('mean absolute error: {0}'.format(mean_absolute_error(y,predicted)))\n",
        "print('r2 score: {0}'.format(r2_score(y,predicted)))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y, predicted, marker='.')\n",
        "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
        "ax.set_xlabel('Real Age')\n",
        "ax.set_ylabel('Predicted Age')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui5VRikXzgjU",
        "colab_type": "text"
      },
      "source": [
        "## Part C: CNN-based regression using grey matter maps\n",
        "\n",
        "The third approach is similar in nature to the second approach in task B, but instead of using PCA for dimensionality reduction in order to use a more classical regression model, now we will use convolutional neural networks (CNNs) on the grey matter maps for predicting the subject's age directly.\n",
        "\n",
        "You will need to implement a CNN model that takes a grey matter map as an input and maps it to a one-dimensional, real-valued output. A good starting point may be a LeNet-type architecture and adapt the last layers to convert the classification into a regression network. You should have all the necessary ingredients now from above tasks and the notebooks from the lab tutorials for how to set up a CNN model in PyTorch, how to implement a suitable training and testing routine, and how to run a two-fold cross-validation on the 500 subjects similar to tasks A and B.\n",
        "\n",
        "*Note:* For part C, only the spatially normalised grey matter maps should be used. Similar to task A, you may want to set up a configuration for the CNN training that may also involve some resampling of the input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9nWMzK1zgjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "# ADD YOUR CODE HERE\n",
        "########################################\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rQNsRjXsIsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the meta data using pandas\n",
        "import pandas as pd\n",
        "\n",
        "# data directory\n",
        "data_dir = 'data/brain_age/'\n",
        "meta_data_reg_train = pd.read_csv(data_dir + 'meta/meta_data_reg_train.csv')\n",
        "meta_data_reg_train.head() # show the first five data entries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6q-TBkZrxVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subject with index 0\n",
        "ID = meta_data_reg_train['subject_id'][0]\n",
        "age = meta_data_reg_train['age'][0]\n",
        "\n",
        "# Grey matter map\n",
        "gm_filename = data_dir + 'greymatter/wc1sub-' + ID + '_T1w.nii.gz'\n",
        "gm = sitk.ReadImage(gm_filename)\n",
        "\n",
        "print('Imaging data of subject ' + ID + ' with age ' + str(age))\n",
        "\n",
        "print('Spatially normalised grey matter maps')\n",
        "display_image(gm)\n",
        "img = sitk.GetArrayFromImage(gm)\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrGSmpNFsfng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resample_image(image, out_spacing=(1.0, 1.0, 1.0), out_size=None, is_label=False, pad_value=0):\n",
        "    \"\"\"Resamples an image to given element spacing and output size.\"\"\"\n",
        "\n",
        "    original_spacing = np.array(image.GetSpacing())\n",
        "    original_size = np.array(image.GetSize())\n",
        "\n",
        "    if out_size is None:\n",
        "        out_size = np.round(np.array(original_size * original_spacing / np.array(out_spacing))).astype(int)\n",
        "    else:\n",
        "        out_size = np.array(out_size)\n",
        "\n",
        "    original_direction = np.array(image.GetDirection()).reshape(len(original_spacing),-1)\n",
        "    original_center = (np.array(original_size, dtype=float) - 1.0) / 2.0 * original_spacing\n",
        "    out_center = (np.array(out_size, dtype=float) - 1.0) / 2.0 * np.array(out_spacing)\n",
        "\n",
        "    original_center = np.matmul(original_direction, original_center)\n",
        "    out_center = np.matmul(original_direction, out_center)\n",
        "    out_origin = np.array(image.GetOrigin()) + (original_center - out_center)\n",
        "\n",
        "    resample = sitk.ResampleImageFilter()\n",
        "    resample.SetOutputSpacing(out_spacing)\n",
        "    resample.SetSize(out_size.tolist())\n",
        "    resample.SetOutputDirection(image.GetDirection())\n",
        "    resample.SetOutputOrigin(out_origin.tolist())\n",
        "    resample.SetTransform(sitk.Transform())\n",
        "    resample.SetDefaultPixelValue(pad_value)\n",
        "\n",
        "    if is_label:\n",
        "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
        "    else:\n",
        "        resample.SetInterpolator(sitk.sitkBSpline)\n",
        "\n",
        "    return resample.Execute(image)\n",
        "\n",
        "\n",
        "class ImageRegressionDataset(Dataset):\n",
        "    \"\"\"Dataset for image segmentation.\"\"\"\n",
        "\n",
        "    def __init__(self, file_list_reg, age, img_spacing, img_size):\n",
        "        self.samples = []\n",
        "        self.img_names = []\n",
        "\n",
        "        for idx, _ in enumerate(tqdm(range(len(file_list_reg)), desc='Loading Data')):\n",
        "            img_path = file_list_reg[idx]\n",
        "\n",
        "            img = sitk.ReadImage(img_path, sitk.sitkFloat32)\n",
        "\n",
        "            #pre=processing\n",
        "            img = resample_image(img, img_spacing, img_size, is_label=False)\n",
        "\n",
        "            sample = {'img': img, 'age': age[idx]}\n",
        "\n",
        "            self.samples.append(sample)\n",
        "            self.img_names.append(os.path.basename(img_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sample = self.samples[item]\n",
        "        \n",
        "        image = torch.from_numpy(sitk.GetArrayFromImage(sample['img'])).unsqueeze(0)\n",
        "        return {'img': image, 'age':sample['age']}\n",
        "\n",
        "    def get_sample(self, item):\n",
        "        return self.samples[item]\n",
        "\n",
        "    def get_img_name(self, item):\n",
        "        return self.img_names[item]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI3ibdLcKSWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda_dev = '0' #GPU device 0 (can be changed if multiple GPUs are available)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:\" + cuda_dev if use_cuda else \"cpu\")\n",
        "\n",
        "print('Device: ' + str(device))\n",
        "if use_cuda:\n",
        "    print('GPU: ' + str(torch.cuda.get_device_name(int(cuda_dev))))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU2F6F-rtAS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnd_seed = 42 #fixed random seed\n",
        "\n",
        "img_size = [64, 64, 64]\n",
        "img_spacing = [3, 3, 3]\n",
        "\n",
        "num_epochs = 100\n",
        "learning_rate = 1e-4\n",
        "batch_size = 6\n",
        "val_interval = 10\n",
        "\n",
        "out_dir = './output'\n",
        "\n",
        "# Create output directory\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2RXkDP4tHNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_data_reg_train = pd.read_csv(data_dir + 'meta/meta_data_reg_train.csv')\n",
        "ids_reg_train = list(meta_data_reg_train['subject_id'])\n",
        "reg_train_age = list(meta_data_reg_train['age'])\n",
        "files_reg_img_train = [data_dir + 'greymatter/wc1sub-' + f + '_T1w.nii.gz' for f in ids_reg_train]\n",
        "\n",
        "train_img, test_img, train_age, test_age = train_test_split(files_reg_img_train, reg_train_age, test_size=0.5, random_state=rnd_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V67bquotdXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e9e1700d-d513-4612-d652-dd1c4ee7ed17"
      },
      "source": [
        "dataset_train = ImageRegressionDataset(train_img, train_age, img_spacing, img_size)\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size)\n",
        "\n",
        "dataset_test = ImageRegressionDataset(test_img, test_age, img_spacing, img_size)\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data: 100%|██████████| 250/250 [00:17<00:00, 14.75it/s]\n",
            "Loading Data: 100%|██████████| 250/250 [00:16<00:00, 14.87it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcTZfYDZuFE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "f2e3afdd-3d3c-4f85-86d6-24c908612f2a"
      },
      "source": [
        "sample = dataset_train.get_sample(0)\n",
        "img_name = dataset_train.get_img_name(0)\n",
        "print('Image: ' + img_name)\n",
        "display_image(sample['img'], window=5, level=0)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image: wc1sub-CC520745_T1w.nii.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAADHCAYAAADbAB3QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKYk6ViIiIiIl+f8lFsMOCx4tmgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHDrf4giuWxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # Make the above architecture of LeNet.\n",
        "        # bias=True makes the layer create a bias internally. Nice and clean.\n",
        "        # Conv2d gets args: (num input channs, num out channs, kernel_size ...)\n",
        "        # Linear gets args: (num input neurons, num out neurons, ...)\n",
        "        self.conv1 = nn.Conv3d(1, 1, kernel_size=2, bias=True, padding=0)\n",
        "        self.conv2 = nn.Conv3d(1, 12, kernel_size=5, bias=True, padding=0)\n",
        "        self.conv3 = nn.Conv3d(12, 32, kernel_size=5, bias=True, padding=0)\n",
        "        self.drop1 = nn.Dropout(p=0.2)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5 * 5, 120, bias=True) \n",
        "        self.fc2 = nn.Linear(120, 84, bias=True)\n",
        "        self.fc3 = nn.Linear(84, 1, bias=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: Input tensor (batch of images) of shape [N, Channels, H, W]\n",
        "        # returns: tensor of shape [N, classes]. The class posterior probabilities.\n",
        "        # Make the forward pass.\n",
        "        x = F.max_pool3d(F.relu(self.conv1(x)), kernel_size=2, stride=2, padding=1, ceil_mode=False)\n",
        "        x = F.max_pool3d(F.relu(self.conv2(x)), kernel_size=2, stride=2, padding=0, ceil_mode=False)\n",
        "        x = F.max_pool3d(F.relu(self.conv3(x)), kernel_size=2, stride=2, padding=0, ceil_mode=False)\n",
        "        x = self.drop1(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        y_pred = self.fc3(x)\n",
        "        ####################################################################\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53PrySQ32fkP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "698736a0-f81f-4979-9ee1-2359b2f74b29"
      },
      "source": [
        "model = LeNet()\n",
        "model.cuda()\n",
        "model.train()\n",
        "    \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_train_log = []\n",
        "loss_val_log = []\n",
        "epoch_val_log = []\n",
        "print('START TRAINING...')\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    # Training\n",
        "    for batch_idx, batch_samples in enumerate(dataloader_train):\n",
        "        img = batch_samples['img'].to(device)\n",
        "        age = batch_samples['age'].type(torch.float).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        prd = model(img)\n",
        "        age = age.reshape(-1, 1)\n",
        "        loss = F.mse_loss(prd, age)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_train_log.append(loss.item())\n",
        "\n",
        "    print('+ TRAINING \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
        "\n",
        "    # Validation\n",
        "    if epoch == 1 or epoch % val_interval == 0:\n",
        "        loss_val = 0\n",
        "        with torch.no_grad():\n",
        "            for data_sample in dataloader_test:\n",
        "                img = data_sample['img'].to(device)\n",
        "                age = data_sample['age'].type(torch.float).to(device)\n",
        "\n",
        "                prd = model(img)\n",
        "                age = age.reshape(-1, 1)\n",
        "                \n",
        "                loss_val += F.mse_loss(prd, age)\n",
        "        \n",
        "        loss_val_log.append(loss_val)\n",
        "        epoch_val_log.append(epoch)\n",
        "\n",
        "        print('--------------------------------------------------')\n",
        "        print('+ VALIDATE \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, loss_val))\n",
        "        print('--------------------------------------------------')\n",
        "\n",
        "print('\\nFinished TRAINING.')\n",
        "\n",
        "plt.plot(range(1, num_epochs + 1), loss_train_log, c='r', label='train')\n",
        "plt.plot(epoch_val_log, loss_val_log, c='b', label='val')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "START TRAINING...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-158-26b06c7ac6ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-157-5a3e9de68b41>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [6 x 32], m2: [4000 x 120] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzHbEcnQ2aqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAm9AnfnzgjZ",
        "colab_type": "text"
      },
      "source": [
        "Error calculation and plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vDeolqdzgjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('mean absolute error: {0}'.format(mean_absolute_error(y,predicted)))\n",
        "print('r2 score: {0}'.format(r2_score(y,predicted)))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y, predicted, marker='.')\n",
        "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
        "ax.set_xlabel('Real Age')\n",
        "ax.set_ylabel('Predicted Age')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}